# main.py (V4: å…¨é¢å‡ç´šè‡³ Google AI - Gemini)
import os
import requests
import feedparser
import logging
import json
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser
from deep_translator import GoogleTranslator
import google.generativeai as genai # NEW: å¼•å…¥ Google AI å‡½å¼åº«

# --- è¨­å®š ---
NOTION_TOKEN = os.getenv('NOTION_TOKEN')
DATABASE_ID = os.getenv('NOTION_DATABASE_ID')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY') # MODIFIED: è®€å– Google API Key
MAX_ENTRIES_PER_RUN = int(os.getenv('MAX_ENTRIES_PER_RUN', '100'))

# ... (RSS_FEEDS, logging, NOTION_API_URL, HEADERS çš„è¨­å®šç¶­æŒä¸è®Š)
RSS_FEEDS = {
    "Bloomberg": "https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine.rss",
    "Hacker News": "https://news.ycombinator.com/rss",
    "TechCrunch": "https://techcrunch.com/feed/",
    "Harvard Business Review": "https://hbr.org/rss/regular",
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
NOTION_API_URL = "https://api.notion.com/v1/"
HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}", "Content-Type": "application/json", "Notion-Version": "2022-06-28",
}

# --- NEW: å…¨æ–°æ”¹å¯«çš„ AI åˆ†æå‡½å¼ (for Gemini) ---
def analyze_article_with_ai(title: str, summary: str) -> dict:
    """ä½¿ç”¨ Google Gemini API åˆ†ææ–‡ç« ï¼Œä¸€æ¬¡å–å¾—å¤šé …è³‡è¨Š"""
    if not GOOGLE_API_KEY:
        logging.warning("âš ï¸ æœªè¨­å®š GOOGLE_API_KEYï¼Œè·³é AI åˆ†æåŠŸèƒ½ã€‚")
        return {}

    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        # Gemini 1.5 Flash æ˜¯é€Ÿåº¦å¿«ã€æˆæœ¬æ•ˆç›Šé«˜çš„æœ€æ–°æ¨¡å‹
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        
        # æˆ‘å€‘çš„é­”æ³•æŒ‡ä»¤
        prompt = f"""
        ä½ æ˜¯ä¸€ä½é ‚å°–çš„å•†æ¥­èˆ‡ç§‘æŠ€åˆ†æå¸«ã€‚è«‹ç°¡æ½”åœ°é–±è®€ä»¥ä¸‹æ–‡ç« çš„æ¨™é¡Œèˆ‡æ‘˜è¦ï¼Œä¸¦åš´æ ¼æŒ‰ç…§ä¸‹é¢çš„ JSON æ ¼å¼å›å‚³åˆ†æçµæœã€‚

        ä½ çš„å›å‚³å¿…é ˆæ˜¯ä¸€å€‹å¯è¢«ç›´æ¥è§£æçš„ JSON ç‰©ä»¶ï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æˆ– Markdown èªæ³• (ä¾‹å¦‚ ```json ```)ã€‚

        JSON æ ¼å¼ç¯„æœ¬å¦‚ä¸‹:
        {{
          "summary": "ä¸€å€‹åŒ…å« 3 å€‹é‡é»çš„ç¹é«”ä¸­æ–‡æ¢åˆ—å¼æ‘˜è¦",
          "keywords": ["æ ¸å¿ƒé—œéµå­—1", "æ ¸å¿ƒé—œéµå­—2", "æ ¸å¿ƒé—œéµå­—3"],
          "sentiment": "æ­£é¢ | è² é¢ | ä¸­æ€§",
          "entities": ["æåŠçš„å…¬å¸æˆ–äººç‰©1", "æåŠçš„å…¬å¸æˆ–äººç‰©2"]
        }}

        ---
        æ–‡ç« æ¨™é¡Œ: "{title}"
        æ–‡ç« æ‘˜è¦: "{summary}"
        ---
        """

        logging.info(f"  ğŸ¤– æ­£åœ¨ç™¼é€è‡³ Google AI (Gemini) é€²è¡Œåˆ†æ: {title}")
        response = model.generate_content(prompt)
        
        # è§£æ AI å›å‚³çš„ JSON å­—ä¸²
        # Gemini æœ‰æ™‚æœƒåœ¨å›å‚³çš„ JSON å‰å¾ŒåŠ ä¸Š ```json ... ```ï¼Œæˆ‘å€‘éœ€è¦å…ˆæ¸…ç†æ‰
        cleaned_json_text = response.text.strip().replace("```json", "").replace("```", "").strip()
        analysis_result = json.loads(cleaned_json_text)

        logging.info("  âœ… AI åˆ†æå®Œæˆã€‚")
        return analysis_result
    except Exception as e:
        logging.error(f"  ğŸ›‘ AI åˆ†æå¤±æ•—: {title} | éŒ¯èª¤: {e}")
        return {} # å³ä½¿å¤±æ•—ï¼Œä¹Ÿå›å‚³ç©ºå­—å…¸ï¼Œä¸ä¸­æ–·ä¸»æµç¨‹

# ... (å…¶ä»–å‡½å¼ç¶­æŒä¸è®Š)
def calculate_reading_time(text: str) -> int:
    if not text: return 0
    word_count = len(text)
    reading_minutes = round(word_count / 300)
    return max(1, reading_minutes)

def check_env_vars():
    if not NOTION_TOKEN or not DATABASE_ID:
        logging.error("ğŸš« ç¼ºå°‘ç’°å¢ƒè®Šæ•¸ï¼šNOTION_TOKEN æˆ– NOTION_DATABASE_ID")
        return False
    return True

def get_existing_urls_from_notion():
    existing_urls = set()
    query_url = f"{NOTION_API_URL}databases/{DATABASE_ID}/query"
    has_more, next_cursor = True, None
    while has_more:
        payload = {"start_cursor": next_cursor} if next_cursor else {}
        try:
            response = requests.post(query_url, headers=HEADERS, json=payload)
            response.raise_for_status()
            data = response.json()
            for page in data.get("results", []):
                url_prop = page.get("properties", {}).get("URL", {})
                if url_prop.get("url"): existing_urls.add(url_prop["url"])
            has_more, next_cursor = data.get("has_more", False), data.get("next_cursor")
        except requests.exceptions.RequestException as e:
            logging.error(f"ğŸ›‘ æŸ¥è©¢ Notion è³‡æ–™åº«å¤±æ•—: {e}")
            return existing_urls
    logging.info(f"âœ… æˆåŠŸå¾ Notion å–å¾—å…¨éƒ¨åˆ†é ï¼Œå…± {len(existing_urls)} ç­†å·²å­˜åœ¨çš„ URL")
    return existing_urls

def add_entry_to_notion(source: str, entry: dict, analysis: dict, reading_time: int):
    create_page_url = f"{NOTION_API_URL}pages"
    title = entry.get("title", "ç„¡æ¨™é¡Œ")
    link = entry.get("link", "")
    published_dt = date_parser.parse(entry.get("published", datetime.now(timezone.utc).isoformat()))
    try:
        translated_title = GoogleTranslator(source='auto', target='zh-TW').translate(title)
    except Exception:
        translated_title = title
    payload = {
        "parent": {"database_id": DATABASE_ID},
        "properties": {
            "æ¨™é¡Œ": {"title": [{"text": {"content": title}}]},
            "ä¸­æ–‡æ¨™é¡Œ": {"rich_text": [{"text": {"content": translated_title}}]},
            "URL": {"url": link},
            "ä¾†æº": {"select": {"name": source}},
            "ç™¼å¸ƒæ™‚é–“": {"date": {"start": published_dt.isoformat()}},
            "AI æ‘˜è¦": {"rich_text": [{"text": {"content": analysis.get("summary", "N/A")}}]},
            "é—œéµå­—": {"multi_select": [{"name": kw} for kw in analysis.get("keywords", [])]},
            "é–±è®€æ™‚é–“(åˆ†)": {"number": reading_time},
            "æƒ…ç·’": {"select": {"name": analysis.get("sentiment", "N/A")} if analysis.get("sentiment") else None},
            "æåŠå¯¦é«”": {"multi_select": [{"name": entity} for entity in analysis.get("entities", [])]}
        }
    }
    try:
        response = requests.post(create_page_url, headers=HEADERS, json=payload)
        response.raise_for_status()
        logging.info(f"âœ… æˆåŠŸæ–°å¢ä¸¦åˆ†ææ–‡ç« åˆ° Notion: {title}")
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"ğŸ›‘ æ–°å¢æ–‡ç« åˆ° Notion å¤±æ•—: {title} | éŒ¯èª¤: {e.response.text}")
        return False

def main():
    logging.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œ RSS to Notion è…³æœ¬ (AIå¼•æ“: Google Gemini)...")
    if not check_env_vars(): return
    thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
    existing_urls = get_existing_urls_from_notion()
    new_entries_count = 0
    all_new_entries = []
    logging.info("--- éšæ®µä¸€: æ”¶é›†æ‰€æœ‰ä¾†æºçš„è¿‘æœŸæ–°æ–‡ç«  ---")
    for source_name, feed_url in RSS_FEEDS.items():
        logging.info(f"ğŸ“¡ æ­£åœ¨è™•ç†ä¾†æº: {source_name}")
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            if entry.link in existing_urls: continue
            published_dt = date_parser.parse(entry.get("published", datetime.now(timezone.utc).isoformat()))
            if published_dt.tzinfo is None: published_dt = published_dt.replace(tzinfo=timezone.utc)
            if published_dt >= thirty_days_ago:
                entry.source_name = source_name
                entry.published_datetime = published_dt
                all_new_entries.append(entry)
    logging.info(f"ğŸ” æ”¶é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_new_entries)} ç­† 30 å¤©å…§çš„æ–°æ–‡ç« ã€‚")
    all_new_entries.sort(key=lambda e: e.published_datetime, reverse=True)
    logging.info(f"--- éšæ®µäºŒ: åˆ†æä¸¦å¯«å…¥è³‡æ–™åº« (å–®æ¬¡ä¸Šé™: {MAX_ENTRIES_PER_RUN} ç­†) ---")
    for entry in all_new_entries:
        if new_entries_count >= MAX_ENTRIES_PER_RUN:
            logging.info(f"ğŸ å·²é”åˆ°å–®æ¬¡åŸ·è¡Œä¸Šé™ï¼Œæå‰çµæŸä»»å‹™ã€‚")
            break
        content_to_analyze = entry.get("summary", "")
        reading_time = calculate_reading_time(content_to_analyze)
        analysis_results = analyze_article_with_ai(entry.get("title", ""), content_to_analyze)
        logging.info(f"âœï¸ æ­£åœ¨è™•ç†æ–‡ç« : {entry.title}")
        if add_entry_to_notion(entry.source_name, entry, analysis_results, reading_time):
            new_entries_count += 1
            existing_urls.add(entry.link)
    logging.info(f"ğŸ‰ ä»»å‹™å®Œæˆï¼æœ¬æ¬¡å…±æ–°å¢ {new_entries_count} ç­†æ–°æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
