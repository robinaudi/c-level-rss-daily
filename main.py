# main.py (V6: æ•´åˆ API æ—¥èªŒå„€è¡¨æ¿åŠŸèƒ½)
import os
import requests
import feedparser
import logging
import json
import time
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser
from deep_translator import GoogleTranslator
import google.generativeai as genai

# --- è¨­å®š ---
NOTION_TOKEN = os.getenv('NOTION_TOKEN')
DATABASE_ID = os.getenv('NOTION_DATABASE_ID')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
LOG_DATABASE_ID = os.getenv('LOG_DATABASE_ID') # NEW: æ–°å¢æ—¥èªŒè³‡æ–™åº« ID
MAX_ENTRIES_PER_RUN = int(os.getenv('MAX_ENTRIES_PER_RUN', '100'))

# ... (RSS_FEEDS, logging, NOTION_API_URL, HEADERS çš„è¨­å®šç¶­æŒä¸è®Š)
RSS_FEEDS = {
    "Bloomberg": "https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine.rss", "Hacker News": "https://news.ycombinator.com/rss", "TechCrunch": "https://techcrunch.com/feed/", "Harvard Business Review": "https://hbr.org/rss/regular",
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
NOTION_API_URL = "https://api.notion.com/v1/"
HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}", "Content-Type": "application/json", "Notion-Version": "2022-06-28",
}

# --- æ ¸å¿ƒå‡½å¼ ---

# MODIFIED: AI åˆ†æå‡½å¼ç¾åœ¨æœƒå›å‚³ token ä½¿ç”¨é‡
def analyze_article_with_ai(title: str, summary: str) -> tuple[dict, int]:
    """ä½¿ç”¨ Google Gemini API åˆ†ææ–‡ç« ï¼Œä¸¦å›å‚³åˆ†æçµæœèˆ‡ token ç”¨é‡"""
    if not GOOGLE_API_KEY:
        logging.warning("âš ï¸ æœªè¨­å®š GOOGLE_API_KEYï¼Œè·³é AI åˆ†æåŠŸèƒ½ã€‚")
        return {}, 0

    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        prompt = f"""
        ä½ æ˜¯ä¸€ä½é ‚å°–çš„å•†æ¥­èˆ‡ç§‘æŠ€åˆ†æå¸«ã€‚è«‹ç°¡æ½”åœ°é–±è®€ä»¥ä¸‹æ–‡ç« çš„æ¨™é¡Œèˆ‡æ‘˜è¦ï¼Œä¸¦åš´æ ¼æŒ‰ç…§ä¸‹é¢çš„ JSON æ ¼å¼å›å‚³åˆ†æçµæœã€‚
        ä½ çš„å›å‚³å¿…é ˆæ˜¯ä¸€å€‹å¯è¢«ç›´æ¥è§£æçš„ JSON ç‰©ä»¶ï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æˆ– Markdown èªæ³•ã€‚
        JSON æ ¼å¼ç¯„æœ¬å¦‚ä¸‹:
        {{
          "summary": "ä¸€å€‹åŒ…å« 3 å€‹é‡é»çš„ç¹é«”ä¸­æ–‡æ¢åˆ—å¼æ‘˜è¦",
          "keywords": ["æ ¸å¿ƒé—œéµå­—1", "æ ¸å¿ƒé—œéµå­—2"],
          "sentiment": "æ­£é¢ | è² é¢ | ä¸­æ€§",
          "entities": ["æåŠçš„å…¬å¸æˆ–äººç‰©1"]
        }}
        ---
        æ–‡ç« æ¨™é¡Œ: "{title}"
        æ–‡ç« æ‘˜è¦: "{summary}"
        ---
        """
        logging.info(f"  ğŸ¤– æ­£åœ¨ç™¼é€è‡³ Google AI (Gemini) é€²è¡Œåˆ†æ: {title}")
        response = model.generate_content(prompt)
        
        # å–å¾— token ç”¨é‡
        token_usage = response.usage_metadata.total_token_count
        
        cleaned_json_text = response.text.strip().replace("```json", "").replace("```", "").strip()
        analysis_result = json.loads(cleaned_json_text)
        logging.info(f"  âœ… AI åˆ†æå®Œæˆ (ä½¿ç”¨ {token_usage} tokens)ã€‚")
        return analysis_result, token_usage
    except Exception as e:
        logging.error(f"  ğŸ›‘ AI åˆ†æå¤±æ•—: {title} | éŒ¯èª¤: {e}")
        return {}, 0

# NEW: å…¨æ–°çš„æ—¥èªŒå¯«å…¥å‡½å¼
def write_log_to_notion(log_data: dict):
    """å°‡å–®æ¬¡é‹è¡Œçš„æ—¥èªŒå¯«å…¥æŒ‡å®šçš„ Notion è³‡æ–™åº«"""
    if not LOG_DATABASE_ID:
        logging.warning("âš ï¸ æœªè¨­å®š LOG_DATABASE_IDï¼Œè·³éå¯«å…¥æ—¥èªŒã€‚")
        return

    page_title = f"API Log: {log_data['source_name']} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    
    payload = {
        "parent": {"database_id": LOG_DATABASE_ID},
        "properties": {
            # **è«‹ç¢ºä¿é€™è£¡çš„å±¬æ€§åç¨±èˆ‡æ‚¨ Notion æ—¥èªŒè³‡æ–™åº«ä¸­çš„å®Œå…¨ä¸€è‡´**
            "æ–°é é¢": {"title": [{"text": {"content": page_title}}]},
            "æ–°èä¾†æº": {"select": {"name": log_data["source_name"]}},
            "æˆåŠŸåˆ†ææ•¸": {"number": log_data["success_count"]},
            "Gemini Tokens": {"number": log_data["gemini_tokens"]},
            "ä»£ç†æœå‹™ (æ¬¡)": {"number": log_data["api_calls"]}
        }
    }
    
    try:
        response = requests.post(f"{NOTION_API_URL}pages", headers=HEADERS, json=payload)
        response.raise_for_status()
        logging.info(f"ğŸ“‹ æˆåŠŸå¯«å…¥æ—¥èªŒåˆ° Notion: {log_data['source_name']}")
    except requests.exceptions.RequestException as e:
        logging.error(f"ğŸ›‘ å¯«å…¥æ—¥èªŒåˆ° Notion å¤±æ•— | éŒ¯èª¤: {e.response.text}")

# ... (å…¶ä»–å‡½å¼ç¶­æŒä¸è®Š)
def calculate_reading_time(text: str) -> int:
    if not text: return 0
    return max(1, round(len(text) / 300))

def check_env_vars():
    if not NOTION_TOKEN or not DATABASE_ID: return False
    return True

def get_existing_urls_from_notion():
    existing_urls = set()
    query_url, has_more, next_cursor = f"{NOTION_API_URL}databases/{DATABASE_ID}/query", True, None
    while has_more:
        payload = {"start_cursor": next_cursor} if next_cursor else {}
        try:
            response = requests.post(query_url, headers=HEADERS, json=payload)
            response.raise_for_status()
            data = response.json()
            for page in data.get("results", []):
                url_prop = page.get("properties", {}).get("URL", {})
                if url_prop.get("url"): existing_urls.add(url_prop["url"])
            has_more, next_cursor = data.get("has_more", False), data.get("next_cursor")
        except requests.exceptions.RequestException: return existing_urls
    logging.info(f"âœ… æˆåŠŸå¾ä¸»è³‡æ–™åº«å–å¾— {len(existing_urls)} ç­†å·²å­˜åœ¨çš„ URL")
    return existing_urls

def add_entry_to_notion(source: str, entry: dict, analysis: dict, reading_time: int):
    # æ­¤å‡½å¼å…§å®¹å®Œå…¨ä¸è®Š
    create_page_url = f"{NOTION_API_URL}pages"
    title, link = entry.get("title", "ç„¡æ¨™é¡Œ"), entry.get("link", "")
    published_dt = date_parser.parse(entry.get("published", datetime.now(timezone.utc).isoformat()))
    try: translated_title = GoogleTranslator(source='auto', target='zh-TW').translate(title)
    except Exception: translated_title = title
    payload = {"parent": {"database_id": DATABASE_ID}, "properties": {
            "æ¨™é¡Œ": {"title": [{"text": {"content": title}}]}, "ä¸­æ–‡æ¨™é¡Œ": {"rich_text": [{"text": {"content": translated_title}}]}, "URL": {"url": link}, "ä¾†æº": {"select": {"name": source}}, "ç™¼å¸ƒæ™‚é–“": {"date": {"start": published_dt.isoformat()}}, "AI æ‘˜è¦": {"rich_text": [{"text": {"content": analysis.get("summary", "N/A")}}]}, "é—œéµå­—": {"multi_select": [{"name": kw} for kw in analysis.get("keywords", [])]}, "é–±è®€æ™‚é–“(åˆ†)": {"number": reading_time}, "æƒ…ç·’": {"select": {"name": analysis.get("sentiment", "N/A")} if analysis.get("sentiment") else None}, "æåŠå¯¦é«”": {"multi_select": [{"name": entity} for entity in analysis.get("entities", [])]}
        }}
    try:
        response = requests.post(create_page_url, headers=HEADERS, json=payload)
        response.raise_for_status()
        logging.info(f"âœ… æˆåŠŸæ–°å¢ä¸¦åˆ†ææ–‡ç« : {title}")
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"ğŸ›‘ æ–°å¢æ–‡ç« å¤±æ•—: {title} | éŒ¯èª¤: {e.response.text}")
        return False

# --- MODIFIED: ä¸»å‡½å¼é‚è¼¯é‡æ§‹ä»¥æ”¯æ´åˆ†ä¾†æºæ—¥èªŒè¨˜éŒ„ ---
def main():
    logging.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œ RSS to Notion è…³æœ¬ (å«æ—¥èªŒè¨˜éŒ„)...")
    if not check_env_vars(): return
    
    thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
    existing_urls = get_existing_urls_from_notion()
    total_new_entries_count = 0

    # MODIFIED: ç¾åœ¨æ˜¯é€ä¸€è™•ç†æ¯å€‹ä¾†æºï¼Œä¸¦åœ¨è™•ç†å®Œç•¢å¾Œå¯«å…¥æ—¥èªŒ
    for source_name, feed_url in RSS_FEEDS.items():
        logging.info(f"--- é–‹å§‹è™•ç†ä¾†æº: {source_name} ---")
        
        # åˆå§‹åŒ–ç•¶å‰ä¾†æºçš„æ—¥èªŒæ•¸æ“š
        log_data = {
            "source_name": source_name, "success_count": 0, "gemini_tokens": 0, "api_calls": 0
        }

        # æ”¶é›†ç•¶å‰ä¾†æºçš„æ–°æ–‡ç« 
        source_new_entries = []
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            if entry.link in existing_urls: continue
            published_dt = date_parser.parse(entry.get("published", datetime.now(timezone.utc).isoformat()))
            if published_dt.tzinfo is None: published_dt = published_dt.replace(tzinfo=timezone.utc)
            if published_dt >= thirty_days_ago:
                entry.published_datetime = published_dt
                source_new_entries.append(entry)
        
        logging.info(f"ğŸ” ä¾†æº {source_name} æ‰¾åˆ° {len(source_new_entries)} ç­†æ–°æ–‡ç« ã€‚")
        source_new_entries.sort(key=lambda e: e.published_datetime, reverse=True)

        # è™•ç†ç•¶å‰ä¾†æºçš„æ–°æ–‡ç« 
        for entry in source_new_entries:
            if total_new_entries_count >= MAX_ENTRIES_PER_RUN:
                logging.info(f"ğŸ å·²é”åˆ°å–®æ¬¡é‹è¡Œç¸½ä¸Šé™ ({MAX_ENTRIES_PER_RUN} ç­†)ï¼Œåœæ­¢è™•ç†æ­¤ä¾†æºã€‚")
                break
            
            content_to_analyze = entry.get("summary", "")
            reading_time = calculate_reading_time(content_to_analyze)
            
            analysis_results, tokens_used = analyze_article_with_ai(entry.get("title", ""), content_to_analyze)
            log_data["api_calls"] += 1
            log_data["gemini_tokens"] += tokens_used

            logging.info(f"âœï¸ æ­£åœ¨å¯«å…¥æ–‡ç« : {entry.title}")
            if add_entry_to_notion(source_name, entry, analysis_results, reading_time):
                log_data["success_count"] += 1
                total_new_entries_count += 1
                existing_urls.add(entry.link)
            
            logging.info("--- ä¼‘æ¯ 2 ç§’ ---")
            time.sleep(2)
        
        # è™•ç†å®Œä¸€å€‹ä¾†æºå¾Œï¼Œå¦‚æœé€²è¡Œäº†ä»»ä½• API å‘¼å«ï¼Œå°±å¯«å…¥æ—¥èªŒ
        if log_data["api_calls"] > 0:
            write_log_to_notion(log_data)
        
        logging.info(f"--- ä¾†æº {source_name} è™•ç†å®Œç•¢ ---")

    logging.info(f"ğŸ‰ ä»»å‹™å®Œæˆï¼æœ¬æ¬¡å…±æ–°å¢ {total_new_entries_count} ç­†æ–°æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
