# main.py
import feedparser 
import requests
from datetime import datetime, timedelta
import os
import sys

# âœ… è®€å–ç’°å¢ƒè®Šæ•¸ï¼ŒåŠ å…¥éŒ¯èª¤æç¤º
try:
    NOTION_TOKEN = os.environ["NOTION_TOKEN"]
    NOTION_DATABASE_ID = os.environ["NOTION_DATABASE_ID"]
    print("âœ… å·²æˆåŠŸè®€å– NOTION_TOKEN å’Œ NOTION_DATABASE_ID")
except KeyError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸ï¼š{e}")
    sys.exit(1)

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

# è¨­å®šè¦è®€å–çš„ RSS æ¸…å–®ï¼ˆè·ä½å°æ‡‰åˆ†é¡ï¼‰
RSS_FEEDS = [
    {"è·ä½": "CEO", "åˆ†é¡": "å…¨çƒè²¡ç¶“", "url": "https://www.bloomberg.com/feed"},
    {"è·ä½": "COO", "åˆ†é¡": "ä¾›æ‡‰éˆå‹•æ…‹", "url": "https://www.supplychaindive.com/rss/"},
    {"è·ä½": "CFO", "åˆ†é¡": "è²¡å‹™èˆ‡æœƒè¨ˆç­–ç•¥", "url": "https://www.cfodive.com/rss/"},
    {"è·ä½": "CTO", "åˆ†é¡": "æŠ€è¡“ç­–ç•¥", "url": "https://feed.infoq.com/"},
    {"è·ä½": "CIO", "åˆ†é¡": "è³‡å®‰æ²»ç†", "url": "https://www.cio.com/index.rss"},
]

now = datetime.utcnow()
seven_days_ago = now - timedelta(days=7)

# æª¢æŸ¥ Notion æ˜¯å¦å·²ç¶“æœ‰è©²æ¢ç›®ï¼ˆç”¨ URL ç•¶å”¯ä¸€éµï¼‰
def is_duplicate(url):
    try:
        search_url = f"https://api.notion.com/v1/databases/{NOTION_DATABASE_ID}/query"
        payload = {
            "filter": {
                "property": "é€£çµ",
                "url": {"equals": url}
            }
        }
        response = requests.post(search_url, headers=HEADERS, json=payload)
        response.raise_for_status()
        data = response.json()
        return len(data.get("results", [])) > 0
    except Exception as e:
        print(f"âš ï¸ æŸ¥é‡å¤±æ•—: {e}")
        return False

# å°‡è³‡æ–™å¯«å…¥ Notion
def create_page(item, è·ä½, åˆ†é¡):
    try:
        pub_date = datetime(*item.published_parsed[:6]).strftime("%Y.%m.%d")
        payload = {
            "parent": {"database_id": NOTION_DATABASE_ID},
            "properties": {
                "ç™»éŒ„æ—¥æœŸ": {"date": {"start": now.strftime("%Y-%m-%d")}},
                "äº‹ä»¶ç™¼ç”Ÿæ—¥æœŸ": {"date": {"start": datetime(*item.published_parsed[:6]).strftime("%Y-%m-%d")}},
                "æ‘˜è¦": {"rich_text": [{"text": {"content": item.get("summary", "")[:500]}}]},
                "æ¨™é¡Œ": {"title": [{"text": {"content": item.title}}]},
                "åˆ†é¡": {"select": {"name": åˆ†é¡}},
                "é©åˆä¸»ç®¡": {"select": {"name": è·ä½}},
                "é€£çµ": {"url": item.link},
                "è©²æ³¨æ„å“ªäº›ï¼Ÿ": {"rich_text": [{"text": {"content": "å¾…è£œå……ï¼ˆå¯ç”¨ AI è‡ªå‹•æ‘˜è¦ç”Ÿæˆï¼‰"}}]}
            }
        }
        res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=payload)
        res.raise_for_status()
        print(f"âœ… å¯«å…¥æˆåŠŸï¼š{item.title}")
        return True
    except Exception as e:
        print(f"âŒ å¯«å…¥å¤±æ•—ï¼š{item.title}ï¼ŒéŒ¯èª¤ï¼š{e}")
        return False

# åŸ·è¡Œä¸»é‚è¼¯
for feed in RSS_FEEDS:
    print(f"ğŸ“¡ è™•ç†ä¾†æºï¼š{feed['url']}")
    d = feedparser.parse(feed["url"])
    for entry in d.entries:
        if hasattr(entry, "published_parsed"):
            pub_dt = datetime(*entry.published_parsed[:6])
            if pub_dt >= seven_days_ago:
                if not is_duplicate(entry.link):
                    create_page(entry, feed["è·ä½"], feed["åˆ†é¡"])
                else:
                    print("ğŸ” è·³éé‡è¤‡ï¼š", entry.title)
