# main.py (V5: æœ€çµ‚å„ªåŒ–ç‰ˆ - åŠ å…¥å»¶é²ä»¥ç¬¦åˆ API é »ç‡é™åˆ¶)
import os
import requests
import feedparser
import logging
import json
import time # NEW: å¼•å…¥ time å‡½å¼åº«
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser
from deep_translator import GoogleTranslator
import google.generativeai as genai

# ... (æ‰€æœ‰è¨­å®šå’Œé™¤äº† main ä»¥å¤–çš„å‡½å¼éƒ½ç¶­æŒä¸è®Šï¼Œæ­¤è™•çœç•¥ä»¥ä¿æŒç°¡æ½”)
# --- è¨­å®š ---
NOTION_TOKEN = os.getenv('NOTION_TOKEN')
DATABASE_ID = os.getenv('NOTION_DATABASE_ID')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
MAX_ENTRIES_PER_RUN = int(os.getenv('MAX_ENTRIES_PER_RUN', '50'))

RSS_FEEDS = {
    "Harvard Business Review": "https://hbr.org/rss/regular",
    "Bloomberg": "https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine.rss",
    "TechCrunch": "https://techcrunch.com/feed/",
    
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
NOTION_API_URL = "https://api.notion.com/v1/"
HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}", "Content-Type": "application/json", "Notion-Version": "2022-06-28",
}

# --- æ ¸å¿ƒå‡½å¼ ---
def analyze_article_with_ai(title: str, summary: str) -> dict:
    if not GOOGLE_API_KEY:
        logging.warning("âš ï¸ æœªè¨­å®š GOOGLE_API_KEYï¼Œè·³é AI åˆ†æåŠŸèƒ½ã€‚")
        return {}
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        prompt = f"""
        ä½ æ˜¯ä¸€ä½é ‚å°–çš„å•†æ¥­èˆ‡ç§‘æŠ€åˆ†æå¸«ã€‚è«‹ç°¡æ½”åœ°é–±è®€ä»¥ä¸‹æ–‡ç« çš„æ¨™é¡Œèˆ‡æ‘˜è¦ï¼Œä¸¦åš´æ ¼æŒ‰ç…§ä¸‹é¢çš„ JSON æ ¼å¼å›å‚³åˆ†æçµæœã€‚
        ä½ çš„å›å‚³å¿…é ˆæ˜¯ä¸€å€‹å¯è¢«ç›´æ¥è§£æçš„ JSON ç‰©ä»¶ï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æˆ– Markdown èªæ³• (ä¾‹å¦‚ ```json ```)ã€‚
        JSON æ ¼å¼ç¯„æœ¬å¦‚ä¸‹:
        {{
          "summary": "ä¸€å€‹åŒ…å« 3 å€‹é‡é»çš„ç¹é«”ä¸­æ–‡æ¢åˆ—å¼æ‘˜è¦",
          "keywords": ["æ ¸å¿ƒé—œéµå­—1", "æ ¸å¿ƒé—œéµå­—2", "æ ¸å¿ƒé—œéµå­—3"],
          "sentiment": "æ­£é¢ | è² é¢ | ä¸­æ€§",
          "entities": ["æåŠçš„å…¬å¸æˆ–äººç‰©1", "æåŠçš„å…¬å¸æˆ–äººç‰©2"]
        }}
        ---
        æ–‡ç« æ¨™é¡Œ: "{title}"
        æ–‡ç« æ‘˜è¦: "{summary}"
        ---
        """
        logging.info(f"  ğŸ¤– æ­£åœ¨ç™¼é€è‡³ Google AI (Gemini) é€²è¡Œåˆ†æ: {title}")
        response = model.generate_content(prompt)
        cleaned_json_text = response.text.strip().replace("```json", "").replace("```", "").strip()
        analysis_result = json.loads(cleaned_json_text)
        logging.info("  âœ… AI åˆ†æå®Œæˆã€‚")
        return analysis_result
    except Exception as e:
        logging.error(f"  ğŸ›‘ AI åˆ†æå¤±æ•—: {title} | éŒ¯èª¤: {e}")
        return {}
        
def calculate_reading_time(text: str) -> int:
    if not text: return 0
    return max(1, round(len(text) / 300))

def check_env_vars():
    if not NOTION_TOKEN or not DATABASE_ID: return False
    return True

def get_existing_urls_from_notion():
    existing_urls = set()
    query_url, has_more, next_cursor = f"{NOTION_API_URL}databases/{DATABASE_ID}/query", True, None
    while has_more:
        payload = {"start_cursor": next_cursor} if next_cursor else {}
        try:
            response = requests.post(query_url, headers=HEADERS, json=payload)
            response.raise_for_status()
            data = response.json()
            for page in data.get("results", []):
                url_prop = page.get("properties", {}).get("URL", {})
                if url_prop.get("url"): existing_urls.add(url_prop["url"])
            has_more, next_cursor = data.get("has_more", False), data.get("next_cursor")
        except requests.exceptions.RequestException: return existing_urls
    logging.info(f"âœ… æˆåŠŸå¾ Notion å–å¾—å…¨éƒ¨åˆ†é ï¼Œå…± {len(existing_urls)} ç­†å·²å­˜åœ¨çš„ URL")
    return existing_urls

def add_entry_to_notion(source: str, entry: dict, analysis: dict, reading_time: int):
    # æ­¤å‡½å¼å…§å®¹å®Œå…¨ä¸è®Š
    create_page_url = f"{NOTION_API_URL}pages"
    title = entry.get("title", "ç„¡æ¨™é¡Œ")
    link = entry.get("link", "")
    published_dt = date_parser.parse(entry.get("published", datetime.now(timezone.utc).isoformat()))
    try:
        translated_title = GoogleTranslator(source='auto', target='zh-TW').translate(title)
    except Exception:
        translated_title = title
    payload = {
        "parent": {"database_id": DATABASE_ID},
        "properties": {
            "æ¨™é¡Œ": {"title": [{"text": {"content": title}}]},
            "ä¸­æ–‡æ¨™é¡Œ": {"rich_text": [{"text": {"content": translated_title}}]},
            "URL": {"url": link},
            "ä¾†æº": {"select": {"name": source}},
            "ç™¼å¸ƒæ™‚é–“": {"date": {"start": published_dt.isoformat()}},
            "AI æ‘˜è¦": {"rich_text": [{"text": {"content": analysis.get("summary", "N/A")}}]},
            "é—œéµå­—": {"multi_select": [{"name": kw} for kw in analysis.get("keywords", [])]},
            "é–±è®€æ™‚é–“(åˆ†)": {"number": reading_time},
            "æƒ…ç·’": {"select": {"name": analysis.get("sentiment", "N/A")} if analysis.get("sentiment") else None},
            "æåŠå¯¦é«”": {"multi_select": [{"name": entity} for entity in analysis.get("entities", [])]}
        }
    }
    try:
        response = requests.post(create_page_url, headers=HEADERS, json=payload)
        response.raise_for_status()
        logging.info(f"âœ… æˆåŠŸæ–°å¢ä¸¦åˆ†ææ–‡ç« åˆ° Notion: {title}")
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"ğŸ›‘ æ–°å¢æ–‡ç« åˆ° Notion å¤±æ•—: {title} | éŒ¯èª¤: {e.response.text}")
        return False

# --- MODIFIED: ä¸»å‡½å¼åŠ å…¥äº† time.sleep() ---
def main():
    logging.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œ RSS to Notion è…³æœ¬ (AIå¼•æ“: Google Gemini)...")
    if not check_env_vars(): return
    
    thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
    existing_urls = get_existing_urls_from_notion()
    new_entries_count = 0
    all_new_entries = []

    logging.info("--- éšæ®µä¸€: æ”¶é›†æ‰€æœ‰ä¾†æºçš„è¿‘æœŸæ–°æ–‡ç«  ---")
    for source_name, feed_url in RSS_FEEDS.items():
        logging.info(f"ğŸ“¡ æ­£åœ¨è™•ç†ä¾†æº: {source_name}")
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            if entry.link in existing_urls: continue
            published_dt = date_parser.parse(entry.get("published", datetime.now(timezone.utc).isoformat()))
            if published_dt.tzinfo is None: published_dt = published_dt.replace(tzinfo=timezone.utc)
            if published_dt >= thirty_days_ago:
                entry.source_name, entry.published_datetime = source_name, published_dt
                all_new_entries.append(entry)
    
    logging.info(f"ğŸ” æ”¶é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_new_entries)} ç­† 30 å¤©å…§çš„æ–°æ–‡ç« ã€‚")
    all_new_entries.sort(key=lambda e: e.published_datetime, reverse=True)

    logging.info(f"--- éšæ®µäºŒ: åˆ†æä¸¦å¯«å…¥è³‡æ–™åº« (å–®æ¬¡ä¸Šé™: {MAX_ENTRIES_PER_RUN} ç­†) ---")
    for entry in all_new_entries:
        if new_entries_count >= MAX_ENTRIES_PER_RUN:
            logging.info(f"ğŸ å·²é”åˆ°å–®æ¬¡åŸ·è¡Œä¸Šé™ï¼Œæå‰çµæŸä»»å‹™ã€‚")
            break
        
        content_to_analyze = entry.get("summary", "")
        reading_time = calculate_reading_time(content_to_analyze)
        analysis_results = analyze_article_with_ai(entry.get("title", ""), content_to_analyze)
        
        logging.info(f"âœï¸ æ­£åœ¨è™•ç†æ–‡ç« : {entry.title}")
        if add_entry_to_notion(entry.source_name, entry, analysis_results, reading_time):
            new_entries_count += 1
            existing_urls.add(entry.link)
        
        # NEW: åœ¨è™•ç†å®Œä¸€ç­†æ–‡ç« å¾Œï¼Œä¸è«–æˆåŠŸå¤±æ•—ï¼Œéƒ½ä¼‘æ¯ 2 ç§’é˜
        logging.info("--- ä¼‘æ¯ 2 ç§’ï¼Œé¿å…è§¸åŠ API é »ç‡ä¸Šé™ ---")
        time.sleep(2)

    logging.info(f"ğŸ‰ ä»»å‹™å®Œæˆï¼æœ¬æ¬¡å…±æ–°å¢ {new_entries_count} ç­†æ–°æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
